{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d761cc-9e3a-4dbf-ac46-0dd6b856fe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['subject', 'body', 'queue', 'priority', 'language'],\n",
      "        num_rows: 13178\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Zunächst über die Python Konsole die benötigten erweiterungen installieren mit:\n",
    "# pip install transformers datasets torch tensorflow accelerate tf-keras\n",
    "# Danach:\n",
    "# python.exe -m pip install --upgrade pip\n",
    "\n",
    "# Verwendetes Learning Dataset:\n",
    "# import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "# path = kagglehub.dataset_download(\"tobiasbueck/multilingual-customer-support-tickets\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Dataset aus den Rohdaten einlesen\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files='trainigsdaten/archive/dataset-tickets-german_normalized_50_5_2.csv')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d6b7364-7872-4c3a-a878-a4e9036430f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Distilbert Modul mit transformers zum tokenizieren der Rohdaten laden\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "modell_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modell_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(modell_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02f3a645-3909-4dbf-b7a7-6b8405ec7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten Tokenisieren\n",
    "def tokenize_function(examples):\n",
    "    combined_texts = [str(subject) + \" \" + str(body) for subject, body in zip(examples[\"subject\"], examples[\"body\"])]\n",
    "    return tokenizer(combined_texts, padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5621252-28f5-4062-a63f-1606cd0c4688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-local and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 unique labels in the 'priority' column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/fom-llm-training-jupyter-projekt/llm-env/lib/python3.10/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/user/fom-llm-training-jupyter-projekt/llm-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4944' max='4944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4944/4944 11:11:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.600200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.364600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.321600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/fom-llm-training-jupyter-projekt/llm-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/user/fom-llm-training-jupyter-projekt/llm-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4944, training_loss=0.4132528752570785, metrics={'train_runtime': 40271.5891, 'train_samples_per_second': 0.982, 'train_steps_per_second': 0.123, 'total_flos': 5237246320035840.0, 'train_loss': 0.4132528752570785, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KI mit Tokenisierten Daten trainieren\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# --- Corrected Data Preparation ---\n",
    "\n",
    "# 1. Convert the 'priority' column to integer IDs.\n",
    "tokenized_datasets = tokenized_datasets.class_encode_column(\"priority\")\n",
    "\n",
    "# 2. Get the number of classes from the 'priority' column's features.\n",
    "num_labels = tokenized_datasets[\"train\"].features[\"priority\"].num_classes\n",
    "print(f\"Found {num_labels} unique labels in the 'priority' column.\")\n",
    "\n",
    "# 3. NOW, rename the 'priority' column to 'labels'.\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"priority\", \"labels\")\n",
    "\n",
    "# Optional: Tidy up by removing columns that are no longer needed\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['subject', 'body', 'queue', 'language'])\n",
    "\n",
    "# --- Model and Trainer Setup ---\n",
    "\n",
    "# Define the model and tokenizer with the CORRECT num_labels\n",
    "modell_name = \"distilbert-local\" # Corrected: Removed the \"/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modell_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(modell_name, num_labels=num_labels)\n",
    "\n",
    "# The rest of your code remains the same\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ergebnisse\",\n",
    "    eval_strategy=\"no\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "id": "d54a0fc5-30fa-4c86-ac45-7a9d4ae1db92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T05:43:09.208172Z",
     "start_time": "2025-10-14T05:41:41.512439Z"
    }
   },
   "source": [
    "# Chatbot erstellen\n",
    "import os\n",
    "import json\n",
    "from transformers import pipeline\n",
    "\n",
    "# --- 1. Load the final trained model ---\n",
    "\n",
    "# Path to your final trained model checkpoint\n",
    "model_path = \"./ergebnisse/checkpoint-4944\"\n",
    "\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "\n",
    "# Create the text-classification pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model_path, tokenizer=model_path)\n",
    "\n",
    "\n",
    "# --- 2. Load the label mapping to understand the output ---\n",
    "\n",
    "# This reads the model's configuration to find the names of your categories\n",
    "config_path = os.path.join(model_path, 'config.json')\n",
    "with open(config_path) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Creates a dictionary to map IDs (like 0, 1, 2...) to names (like 'High', 'Low'...)\n",
    "id2label = {int(k): v for k, v in config['id2label'].items()}\n",
    "\n",
    "\n",
    "# --- 3. Start the interactive session ---\n",
    "\n",
    "print(\"\\n✅ Interactive session with the classifier has started!\")\n",
    "print(f\"The model will classify text into these categories: {list(id2label.values())}\")\n",
    "print(\"Type a sentence and press Enter, or type 'exit' to end.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Get input from the user\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        # Check for exit command\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"ende\"]:\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        if not user_input.strip(): # Skip empty input\n",
    "            continue\n",
    "            \n",
    "        # Make a prediction\n",
    "        prediction = classifier(user_input)[0]\n",
    "        \n",
    "        # Get the meaningful label and score\n",
    "        label_id = int(prediction['label'].split('_')[1])\n",
    "        predicted_label_name = id2label[label_id]\n",
    "        confidence_score = prediction['score']\n",
    "        \n",
    "        # Display the result\n",
    "        print(f\"Bot: I classify that as '{predicted_label_name}' (Confidence: {confidence_score:.2%})\")\n",
    "\n",
    "    except (KeyboardInterrupt, EOFError): # Handle Ctrl+C or Ctrl+D\n",
    "        print(\"\\nBot: Goodbye!\")\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\musicfiler\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ./ergebnisse/checkpoint-4944\n",
      "WARNING:tensorflow:From C:\\Users\\musicfiler\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Interactive session with the classifier has started!\n",
      "The model will classify text into these categories: ['LABEL_0', 'LABEL_1', 'LABEL_2', 'LABEL_3', 'LABEL_4']\n",
      "Type a sentence and press Enter, or type 'exit' to end.\n",
      "--------------------------------------------------\n",
      "Bot: I classify that as 'LABEL_4' (Confidence: 36.97%)\n",
      "\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e3fcf1638440c361"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
